PID to-do:
- di kode getdata_waqi.py (V)
-- tokennya dipisahin dari url (V)
-- tempatnya dipisahin dari url (V)
-- tempatnya jgn hanya jkt doang (masih kurang stasiun) (V)
--o (opsional) bisa gak nyari stasiun secara automatis? pake koordinat diatas? kalau ngga manual aja.

- (V) buat kode getdata_weatherAPI.py 
-- referensi pake yg ada di kompar aja, tapi jgn dijadiin xlsx tp .csv (V)

preprocessing
- utk date/tanggal itu bisa diganti ke tanggal ambil dat atidak, bukan tanggal data di proses (V)

storage mongodb
= untuk penyimpanan data tinggal run store_data.py di terminal kedua
-o (opsional) di taruh ke file docker-compose biar otomatis (servisnya ada tapi gamau aktif) 
-- di store_data.py cuman nyimpan weather API doang (kurang kode penyimpanan data waqi + daity_risk) (V)
-- store_data.py : dilakukan per jam (per 5 menit aja, for now V)
-- buat satu cvs lagi yg ngitung rata2 risk utk satu pulau jawa (V)
-- automatis producing cvs processed data (di-bound sama store_data wkwkw V)
--- eh ini termasuk ETL atau ELT?? processed-nya setelah getdata soale

o di docker-compose producernya dijadiin satu bisa?
o container getdata bisa diambil sesuai 5 mnt ga

dashboard streamlit
-- lihat lagi prosessed_daily_risk, ada yg kurang tidak (mungkin cuaca/kondisi)
-- cara ngasih data lgsg dr mongodb, bukan dr csv
--- test bisa atau nggak
-- grafik risk_score, per baris satu wilayah
--- utk skrg gausah kasih tren risk, kasih data per satu pulau aja

yg ditanya ke bapaknya:
- data wilayahnya di waqi kurang, disesuaiin atau gimana? (pulau jawa) (V)
- preprocessing pakai python bisa atau harus pakai airflow? (python bisa yayyy) (V)
- date di kode streamlit (pakai agent AI(bruh why)) (V)